{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S1_2_Training_Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Etienne-98/2022_ML_EES/blob/main/Labs/S1_2_Training_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Week 1 Notebook 2 ‚Äì Training Models**\n",
        "\n",
        "This week's notebook is based off of the exercises in Chapter 4 of G√©ron's book."
      ],
      "metadata": {
        "id": "5Tt5C4PoIRl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Setup\n",
        "Let's begin like in the last notebook: importing a few common modules, ensuring MatplotLib plots figures inline and preparing a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so once again we strongly recommend you use Python 3 instead), as well as Scikit-Learn ‚â•0.20.\n",
        "\n",
        "You don't need to worry about understanding everything that is written in this section."
      ],
      "metadata": {
        "id": "666iBNeL8-7H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S_OXSp49IOF2"
      },
      "outputs": [],
      "source": [
        "#@title  Run this cell for preliminary requirements. Double click it if you want to check out the source :)\n",
        "\n",
        "# Python ‚â•3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "# Scikit-Learn ‚â•0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To make this notebook's output stable across runs\n",
        "rnd_seed = 42\n",
        "rnd_gen = np.random.default_rng(rnd_seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"classification\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "#Ensure the palmerspenguins dataset is installed\n",
        "%pip install palmerpenguins --quiet\n",
        "# to have a glimpse of the dataset... it's a dataframe 344x8\n",
        "#> Rows: 344\n",
        "#> Columns: 8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Setup"
      ],
      "metadata": {
        "id": "RtuO7Elb9LuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will be working with the [*Palmer Penguins dataset*](https://allisonhorst.github.io/palmerpenguins/articles/intro.html). Each entry in the dataset includes the penguin's species, island, sex, flipper length, body mass, bill length, bill depth, and the year the study was carried out. Let's take a moment and observe our subjects! <br>\n",
        "\n",
        "<center> <font size=+30>üêß</font><br>\n",
        "In order: Ad√©lie (Pygoscelis adeliae),  Chinstrap (Pygoscelis antarcticus), and Gentoo (Pygoscelis papua) penguins <br>\n",
        "\n",
        "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/EY2OrrxSOSlIiQZ4FQ719YAB80_joiBs9e58jtlSf4H_eQ?download=1' width=32% >\n",
        "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/EVuvzIGBs_JGihnUSWBGQ1IBjv-ZtUDUo7cXeWtyx9g6Og?download=1' width=32%></img>\n",
        "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/EXrZM1rJOXRLlmdPTm-BP2gBCgs1qfQiknp29lX4p_7GtQ?download=1' width=32%></img>\n",
        "\n",
        "</center>\n",
        "\n",
        "As you can imagine, this dataset is normally used to train *multiclass*/*multinomial* classification algorithms and not *binary* classification algorithms, since there *are* more than 2 classes. \n",
        "\n",
        "\"*Three classes, even!*\" - an observant TA\n",
        "\n",
        "For this exercise, however, we will implement the binary classification algorithm referred to as the *logistic regression* algorithm (also called logit regression)."
      ],
      "metadata": {
        "id": "wKsvLXdmzqD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the Palmer Penguins Dataset!\n",
        "from palmerpenguins import load_penguins\n",
        "data = load_penguins()\n",
        "print(data)\n",
        "print(\"Let's take a look at our first few rows:\\n\",\n",
        "      data.head())"
      ],
      "metadata": {
        "id": "emWru72owjEI",
        "outputId": "c9557b38-1844-4fa7-e41a-5d0a5960a9f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
            "0       Adelie  Torgersen            39.1           18.7              181.0   \n",
            "1       Adelie  Torgersen            39.5           17.4              186.0   \n",
            "2       Adelie  Torgersen            40.3           18.0              195.0   \n",
            "3       Adelie  Torgersen             NaN            NaN                NaN   \n",
            "4       Adelie  Torgersen            36.7           19.3              193.0   \n",
            "..         ...        ...             ...            ...                ...   \n",
            "339  Chinstrap      Dream            55.8           19.8              207.0   \n",
            "340  Chinstrap      Dream            43.5           18.1              202.0   \n",
            "341  Chinstrap      Dream            49.6           18.2              193.0   \n",
            "342  Chinstrap      Dream            50.8           19.0              210.0   \n",
            "343  Chinstrap      Dream            50.2           18.7              198.0   \n",
            "\n",
            "     body_mass_g     sex  year  \n",
            "0         3750.0    male  2007  \n",
            "1         3800.0  female  2007  \n",
            "2         3250.0  female  2007  \n",
            "3            NaN     NaN  2007  \n",
            "4         3450.0  female  2007  \n",
            "..           ...     ...   ...  \n",
            "339       4000.0    male  2009  \n",
            "340       3400.0  female  2009  \n",
            "341       3775.0    male  2009  \n",
            "342       4100.0    male  2009  \n",
            "343       3775.0  female  2009  \n",
            "\n",
            "[344 rows x 8 columns]\n",
            "Let's take a look at our first few rows:\n",
            "   species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
            "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
            "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
            "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
            "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
            "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
            "\n",
            "   body_mass_g     sex  year  \n",
            "0       3750.0    male  2007  \n",
            "1       3800.0  female  2007  \n",
            "2       3250.0  female  2007  \n",
            "3          NaN     NaN  2007  \n",
            "4       3450.0  female  2007  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like with the Titanic dataset in the previous notebook, the data here is loaded as a Pandas DataFrame. Feel free to play around with it in the cell below!"
      ],
      "metadata": {
        "id": "kbk8zvwOf2-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.groupby.groupby import DataFrame\n",
        "DataFrame(data)"
      ],
      "metadata": {
        "id": "9ixTqNiqYFvx",
        "outputId": "e4604ad5-a7fe-4897-cec4-1077537e47fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
              "0       Adelie  Torgersen            39.1           18.7              181.0   \n",
              "1       Adelie  Torgersen            39.5           17.4              186.0   \n",
              "2       Adelie  Torgersen            40.3           18.0              195.0   \n",
              "3       Adelie  Torgersen             NaN            NaN                NaN   \n",
              "4       Adelie  Torgersen            36.7           19.3              193.0   \n",
              "..         ...        ...             ...            ...                ...   \n",
              "339  Chinstrap      Dream            55.8           19.8              207.0   \n",
              "340  Chinstrap      Dream            43.5           18.1              202.0   \n",
              "341  Chinstrap      Dream            49.6           18.2              193.0   \n",
              "342  Chinstrap      Dream            50.8           19.0              210.0   \n",
              "343  Chinstrap      Dream            50.2           18.7              198.0   \n",
              "\n",
              "     body_mass_g     sex  year  \n",
              "0         3750.0    male  2007  \n",
              "1         3800.0  female  2007  \n",
              "2         3250.0  female  2007  \n",
              "3            NaN     NaN  2007  \n",
              "4         3450.0  female  2007  \n",
              "..           ...     ...   ...  \n",
              "339       4000.0    male  2009  \n",
              "340       3400.0  female  2009  \n",
              "341       3775.0    male  2009  \n",
              "342       4100.0    male  2009  \n",
              "343       3775.0  female  2009  \n",
              "\n",
              "[344 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-306be561-3beb-4d58-a8c8-333f314b5375\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>species</th>\n",
              "      <th>island</th>\n",
              "      <th>bill_length_mm</th>\n",
              "      <th>bill_depth_mm</th>\n",
              "      <th>flipper_length_mm</th>\n",
              "      <th>body_mass_g</th>\n",
              "      <th>sex</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>39.1</td>\n",
              "      <td>18.7</td>\n",
              "      <td>181.0</td>\n",
              "      <td>3750.0</td>\n",
              "      <td>male</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>39.5</td>\n",
              "      <td>17.4</td>\n",
              "      <td>186.0</td>\n",
              "      <td>3800.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>40.3</td>\n",
              "      <td>18.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>3250.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>36.7</td>\n",
              "      <td>19.3</td>\n",
              "      <td>193.0</td>\n",
              "      <td>3450.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>Chinstrap</td>\n",
              "      <td>Dream</td>\n",
              "      <td>55.8</td>\n",
              "      <td>19.8</td>\n",
              "      <td>207.0</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>male</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>Chinstrap</td>\n",
              "      <td>Dream</td>\n",
              "      <td>43.5</td>\n",
              "      <td>18.1</td>\n",
              "      <td>202.0</td>\n",
              "      <td>3400.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>Chinstrap</td>\n",
              "      <td>Dream</td>\n",
              "      <td>49.6</td>\n",
              "      <td>18.2</td>\n",
              "      <td>193.0</td>\n",
              "      <td>3775.0</td>\n",
              "      <td>male</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>Chinstrap</td>\n",
              "      <td>Dream</td>\n",
              "      <td>50.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>4100.0</td>\n",
              "      <td>male</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>Chinstrap</td>\n",
              "      <td>Dream</td>\n",
              "      <td>50.2</td>\n",
              "      <td>18.7</td>\n",
              "      <td>198.0</td>\n",
              "      <td>3775.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>344 rows √ó 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-306be561-3beb-4d58-a8c8-333f314b5375')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-306be561-3beb-4d58-a8c8-333f314b5375 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-306be561-3beb-4d58-a8c8-333f314b5375');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code will make the dataframe be shown in an interactive table\n",
        "# inside of Google colab. Use data.head(5) if you're running this locally\n",
        "\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "data\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "AR2NqgIdTcuk",
        "outputId": "7f2191cf-627d-46f4-ab55-cf5683de9c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
              "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
              "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
              "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
              "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
              "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
              "\n",
              "   body_mass_g     sex  year  \n",
              "0       3750.0    male  2007  \n",
              "1       3800.0  female  2007  \n",
              "2       3250.0  female  2007  \n",
              "3          NaN     NaN  2007  \n",
              "4       3450.0  female  2007  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c28994b4-281b-4e68-bef3-ef6c5e1099e1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>species</th>\n",
              "      <th>island</th>\n",
              "      <th>bill_length_mm</th>\n",
              "      <th>bill_depth_mm</th>\n",
              "      <th>flipper_length_mm</th>\n",
              "      <th>body_mass_g</th>\n",
              "      <th>sex</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>39.1</td>\n",
              "      <td>18.7</td>\n",
              "      <td>181.0</td>\n",
              "      <td>3750.0</td>\n",
              "      <td>male</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>39.5</td>\n",
              "      <td>17.4</td>\n",
              "      <td>186.0</td>\n",
              "      <td>3800.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>40.3</td>\n",
              "      <td>18.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>3250.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>36.7</td>\n",
              "      <td>19.3</td>\n",
              "      <td>193.0</td>\n",
              "      <td>3450.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c28994b4-281b-4e68-bef3-ef6c5e1099e1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c28994b4-281b-4e68-bef3-ef6c5e1099e1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c28994b4-281b-4e68-bef3-ef6c5e1099e1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a8bd4d5e58f96183/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"Adelie\",\n\"Torgersen\",\n{\n            'v': 39.1,\n            'f': \"39.1\",\n        },\n{\n            'v': 18.7,\n            'f': \"18.7\",\n        },\n{\n            'v': 181.0,\n            'f': \"181.0\",\n        },\n{\n            'v': 3750.0,\n            'f': \"3750.0\",\n        },\n\"male\",\n{\n            'v': 2007,\n            'f': \"2007\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"Adelie\",\n\"Torgersen\",\n{\n            'v': 39.5,\n            'f': \"39.5\",\n        },\n{\n            'v': 17.4,\n            'f': \"17.4\",\n        },\n{\n            'v': 186.0,\n            'f': \"186.0\",\n        },\n{\n            'v': 3800.0,\n            'f': \"3800.0\",\n        },\n\"female\",\n{\n            'v': 2007,\n            'f': \"2007\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"Adelie\",\n\"Torgersen\",\n{\n            'v': 40.3,\n            'f': \"40.3\",\n        },\n{\n            'v': 18.0,\n            'f': \"18.0\",\n        },\n{\n            'v': 195.0,\n            'f': \"195.0\",\n        },\n{\n            'v': 3250.0,\n            'f': \"3250.0\",\n        },\n\"female\",\n{\n            'v': 2007,\n            'f': \"2007\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"Adelie\",\n\"Torgersen\",\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\nNaN,\n{\n            'v': 2007,\n            'f': \"2007\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"Adelie\",\n\"Torgersen\",\n{\n            'v': 36.7,\n            'f': \"36.7\",\n        },\n{\n            'v': 19.3,\n            'f': \"19.3\",\n        },\n{\n            'v': 193.0,\n            'f': \"193.0\",\n        },\n{\n            'v': 3450.0,\n            'f': \"3450.0\",\n        },\n\"female\",\n{\n            'v': 2007,\n            'f': \"2007\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"species\"], [\"string\", \"island\"], [\"number\", \"bill_length_mm\"], [\"number\", \"bill_depth_mm\"], [\"number\", \"flipper_length_mm\"], [\"number\", \"body_mass_g\"], [\"string\", \"sex\"], [\"number\", \"year\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we mentioned before, there are three species of penguin in the dataset. However, today we'll be implementing a _binary classification algorithm_, which means we need to have exactly two target classes! Let's go ahead and filter the data so that we keep the Adelie and Gentoo species."
      ],
      "metadata": {
        "id": "nDFjkxZE1BA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define the species that we're interested in\n",
        "species = ['Adelie','Gentoo']\n",
        "\n",
        "# And use the .loc method in Pandas to keep only the two species mentioned above\n",
        "data = data.loc[data['species'].isin(species)]\n",
        "data.head #shows a 275x8 dataframe; that's logical, for we removed poor Chinstrap penguin.."
      ],
      "metadata": {
        "id": "kTUHBGUT1dTY",
        "outputId": "11de1cab-e442-4575-f79a-70b3f3936c9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of     species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
              "0    Adelie  Torgersen            39.1           18.7              181.0   \n",
              "1    Adelie  Torgersen            39.5           17.4              186.0   \n",
              "2    Adelie  Torgersen            40.3           18.0              195.0   \n",
              "3    Adelie  Torgersen             NaN            NaN                NaN   \n",
              "4    Adelie  Torgersen            36.7           19.3              193.0   \n",
              "..      ...        ...             ...            ...                ...   \n",
              "271  Gentoo     Biscoe             NaN            NaN                NaN   \n",
              "272  Gentoo     Biscoe            46.8           14.3              215.0   \n",
              "273  Gentoo     Biscoe            50.4           15.7              222.0   \n",
              "274  Gentoo     Biscoe            45.2           14.8              212.0   \n",
              "275  Gentoo     Biscoe            49.9           16.1              213.0   \n",
              "\n",
              "     body_mass_g     sex  year  \n",
              "0         3750.0    male  2007  \n",
              "1         3800.0  female  2007  \n",
              "2         3250.0  female  2007  \n",
              "3            NaN     NaN  2007  \n",
              "4         3450.0  female  2007  \n",
              "..           ...     ...   ...  \n",
              "271          NaN     NaN  2009  \n",
              "272       4850.0  female  2009  \n",
              "273       5750.0    male  2009  \n",
              "274       5200.0  female  2009  \n",
              "275       5400.0    male  2009  \n",
              "\n",
              "[276 rows x 8 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Today, we'll be learning to classify the penguins based on the length and depth of their bills.  Run the cell and take a look at the data! üîé\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "# Dimensions for interactive plot\n",
        "dims   = ['bill_length_mm', 'bill_depth_mm']\n",
        "colors = ['orange','black','lightseagreen']\n",
        "\n",
        "fig = px.scatter_matrix(\n",
        "                        data,            # our dataframe\n",
        "                        dimensions=dims, # 2 dimensions that we retained  = bill length and bill depth\n",
        "                        color=\"species\", # colors according to species = ADELIE and GENTOO\n",
        "                        color_discrete_sequence = colors\n",
        "                        )\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "fa4m4iPxgpu9",
        "outputId": "fbcfe41d-f81d-46cd-d78d-7260da453cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"f9464f4d-fa0a-47c0-9753-aa9b8e7783d9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f9464f4d-fa0a-47c0-9753-aa9b8e7783d9\")) {                    Plotly.newPlot(                        \"f9464f4d-fa0a-47c0-9753-aa9b8e7783d9\",                        [{\"dimensions\":[{\"axis\":{\"matches\":true},\"label\":\"bill_length_mm\",\"values\":[39.1,39.5,40.3,null,36.7,39.3,38.9,39.2,34.1,42.0,37.8,37.8,41.1,38.6,34.6,36.6,38.7,42.5,34.4,46.0,37.8,37.7,35.9,38.2,38.8,35.3,40.6,40.5,37.9,40.5,39.5,37.2,39.5,40.9,36.4,39.2,38.8,42.2,37.6,39.8,36.5,40.8,36.0,44.1,37.0,39.6,41.1,37.5,36.0,42.3,39.6,40.1,35.0,42.0,34.5,41.4,39.0,40.6,36.5,37.6,35.7,41.3,37.6,41.1,36.4,41.6,35.5,41.1,35.9,41.8,33.5,39.7,39.6,45.8,35.5,42.8,40.9,37.2,36.2,42.1,34.6,42.9,36.7,35.1,37.3,41.3,36.3,36.9,38.3,38.9,35.7,41.1,34.0,39.6,36.2,40.8,38.1,40.3,33.1,43.2,35.0,41.0,37.7,37.8,37.9,39.7,38.6,38.2,38.1,43.2,38.1,45.6,39.7,42.2,39.6,42.7,38.6,37.3,35.7,41.1,36.2,37.7,40.2,41.4,35.2,40.6,38.8,41.5,39.0,44.1,38.5,43.1,36.8,37.5,38.1,41.1,35.6,40.2,37.0,39.7,40.2,40.6,32.1,40.7,37.3,39.0,39.2,36.6,36.0,37.8,36.0,41.5]},{\"axis\":{\"matches\":true},\"label\":\"bill_depth_mm\",\"values\":[18.7,17.4,18.0,null,19.3,20.6,17.8,19.6,18.1,20.2,17.1,17.3,17.6,21.2,21.1,17.8,19.0,20.7,18.4,21.5,18.3,18.7,19.2,18.1,17.2,18.9,18.6,17.9,18.6,18.9,16.7,18.1,17.8,18.9,17.0,21.1,20.0,18.5,19.3,19.1,18.0,18.4,18.5,19.7,16.9,18.8,19.0,18.9,17.9,21.2,17.7,18.9,17.9,19.5,18.1,18.6,17.5,18.8,16.6,19.1,16.9,21.1,17.0,18.2,17.1,18.0,16.2,19.1,16.6,19.4,19.0,18.4,17.2,18.9,17.5,18.5,16.8,19.4,16.1,19.1,17.2,17.6,18.8,19.4,17.8,20.3,19.5,18.6,19.2,18.8,18.0,18.1,17.1,18.1,17.3,18.9,18.6,18.5,16.1,18.5,17.9,20.0,16.0,20.0,18.6,18.9,17.2,20.0,17.0,19.0,16.5,20.3,17.7,19.5,20.7,18.3,17.0,20.5,17.0,18.6,17.2,19.8,17.0,18.5,15.9,19.0,17.6,18.3,17.1,18.0,17.9,19.2,18.5,18.5,17.6,17.5,17.5,20.1,16.5,17.9,17.1,17.2,15.5,17.0,16.8,18.7,18.6,18.4,17.8,18.1,17.1,18.5]}],\"hovertemplate\":\"species=Adelie<br>%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<extra></extra>\",\"legendgroup\":\"Adelie\",\"marker\":{\"color\":\"orange\",\"symbol\":\"circle\"},\"name\":\"Adelie\",\"showlegend\":true,\"type\":\"splom\"},{\"dimensions\":[{\"axis\":{\"matches\":true},\"label\":\"bill_length_mm\",\"values\":[46.1,50.0,48.7,50.0,47.6,46.5,45.4,46.7,43.3,46.8,40.9,49.0,45.5,48.4,45.8,49.3,42.0,49.2,46.2,48.7,50.2,45.1,46.5,46.3,42.9,46.1,44.5,47.8,48.2,50.0,47.3,42.8,45.1,59.6,49.1,48.4,42.6,44.4,44.0,48.7,42.7,49.6,45.3,49.6,50.5,43.6,45.5,50.5,44.9,45.2,46.6,48.5,45.1,50.1,46.5,45.0,43.8,45.5,43.2,50.4,45.3,46.2,45.7,54.3,45.8,49.8,46.2,49.5,43.5,50.7,47.7,46.4,48.2,46.5,46.4,48.6,47.5,51.1,45.2,45.2,49.1,52.5,47.4,50.0,44.9,50.8,43.4,51.3,47.5,52.1,47.5,52.2,45.5,49.5,44.5,50.8,49.4,46.9,48.4,51.1,48.5,55.9,47.2,49.1,47.3,46.8,41.7,53.4,43.3,48.1,50.5,49.8,43.5,51.5,46.2,55.1,44.5,48.8,47.2,null,46.8,50.4,45.2,49.9]},{\"axis\":{\"matches\":true},\"label\":\"bill_depth_mm\",\"values\":[13.2,16.3,14.1,15.2,14.5,13.5,14.6,15.3,13.4,15.4,13.7,16.1,13.7,14.6,14.6,15.7,13.5,15.2,14.5,15.1,14.3,14.5,14.5,15.8,13.1,15.1,14.3,15.0,14.3,15.3,15.3,14.2,14.5,17.0,14.8,16.3,13.7,17.3,13.6,15.7,13.7,16.0,13.7,15.0,15.9,13.9,13.9,15.9,13.3,15.8,14.2,14.1,14.4,15.0,14.4,15.4,13.9,15.0,14.5,15.3,13.8,14.9,13.9,15.7,14.2,16.8,14.4,16.2,14.2,15.0,15.0,15.6,15.6,14.8,15.0,16.0,14.2,16.3,13.8,16.4,14.5,15.6,14.6,15.9,13.8,17.3,14.4,14.2,14.0,17.0,15.0,17.1,14.5,16.1,14.7,15.7,15.8,14.6,14.4,16.5,15.0,17.0,15.5,15.0,13.8,16.1,14.7,15.8,14.0,15.1,15.2,15.9,15.2,16.3,14.1,16.0,15.7,16.2,13.7,null,14.3,15.7,14.8,16.1]}],\"hovertemplate\":\"species=Gentoo<br>%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<extra></extra>\",\"legendgroup\":\"Gentoo\",\"marker\":{\"color\":\"black\",\"symbol\":\"circle\"},\"name\":\"Gentoo\",\"showlegend\":true,\"type\":\"splom\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"legend\":{\"title\":{\"text\":\"species\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"dragmode\":\"select\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f9464f4d-fa0a-47c0-9753-aa9b8e7783d9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a dataframe with all the information that we need. Let's go ahead and extract the bill length and depth to use as input data, storing it in $x$. \n",
        "Then we'll store the labels (i.e., the _targets_) in $y$.\n",
        "### **Q1) Extract the bill length and bill depth to use as the input vector $x$, and store the label (i.e., the target data) in $y$**\n"
      ],
      "metadata": {
        "id": "kGI9Ruq6BRPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hints - Data Loading and Filtering\n",
        "\n",
        "'''\n",
        "Loading data into X:\n",
        "\n",
        "You can access multiple columns of a pandas dataframe using a list! The snippet\n",
        "below will return the species and island associated with each penguin in the\n",
        "database. \n",
        "\n",
        "In the cell below, you want to load the bill length and bill depth columns.\n",
        "Make sure you use the right column name! Copy it from the dataframe view we\n",
        "printed before, and make sure there aren't any extra spaces\n",
        "--> bill_length_mm and bill_depth_mm\n",
        "''';\n",
        "data[['species','island']];\n",
        "\n",
        "'''\n",
        "Finding the NaN row indices\n",
        "\n",
        "Pandas has a built-in function to determine if the value is a NaN (Not a Number)\n",
        "value. \n",
        "\n",
        "mydata.notna() will return True wherever the data isn't a NaN value, but we need\n",
        "to check if each row has _any_ NaN values - that's what the _all(axis=1)_ does.\n",
        "''';"
      ],
      "metadata": {
        "id": "9kwuG96d5GrS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the bill length and depth into X\n",
        "X = data[['bill_length_mm','bill_depth_mm']]\n",
        "\n",
        "# Find out the rows where you don't have a valid input (i.e., rows with a naN value)\n",
        "indices = X.notna().all(axis=1)\n",
        "\n",
        "# Filter out the datapoints using the indices we found\n",
        "X = X[indices]\n",
        "\n",
        "# We'll also normalize the data using the mean and standard deviation\n",
        "X = (X - X.mean())/X.std()\n",
        "print(X.head(5))\n"
      ],
      "metadata": {
        "id": "zjwWEZhgBQuR",
        "outputId": "424fc93f-f281-4ad7-fadd-42f9d5ae3eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   bill_length_mm  bill_depth_mm\n",
            "0       -0.693460       0.925728\n",
            "1       -0.616472       0.280057\n",
            "2       -0.462494       0.578059\n",
            "4       -1.155393       1.223729\n",
            "5       -0.654966       1.869400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at the input dataset - if you did everything right, you'll \n",
        "# have 274 entries and printing out X.shape will return (274,2)\n",
        "print('The shape of the dataframe X is:\\n', \n",
        "      X.shape) # all right, the size is good: (274,2)"
      ],
      "metadata": {
        "id": "0IoLpmkLugIR",
        "outputId": "c80889ec-ff6b-43b0-d6f0-1a8df990deeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the dataframe X is:\n",
            " (274, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have our input data, but we need a target to predict. We previously filtered the data to only include Ad√©lie and Gentoo penguins, but we still have them as strings! Let's convert them to a binary representation (i.e., 0 or 1). Make sure you have the same penguins as in your input!\n",
        "\n",
        "### **Q2) Convert the species label to a binary classification, and filter the target data to match the input data.**"
      ],
      "metadata": {
        "id": "S63cMjGKLpd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hints - Boolean Representation & Type Conversion\n",
        "\n",
        "''' \n",
        "Boolean Representation\n",
        "\n",
        "You can access the species data by calling data['species']\n",
        "\n",
        "== is the operator that lets you check if the data is equal to another value\n",
        "\n",
        "data['island'] == Torgesen \n",
        "will return True for each row if the penguin was studied in Torgesen, and False\n",
        "if it was studied in another island\n",
        "''';\n",
        "\n",
        "'''\n",
        "Type Conversion\n",
        "\n",
        "Pandas dataframes include a method to change the type of the data being called.\n",
        "\n",
        "data['bill_length_mm'].astype(int) will return the bill length data as integers\n",
        "''';"
      ],
      "metadata": {
        "id": "s3psibnt08RZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the species data into boolean form by checking if the species is Adelie\n",
        "y = (data['species'] == 'Adelie')\n",
        "\n",
        "# Filter out the points for which we have NaN values. Reuse the indices from Q1! -->indices = X.notna().all(axis=1)\n",
        "y = y[indices]\n",
        "\n",
        "# Convert the boolean data into an integer --> 'int'\n",
        "y = y.astype(int) # identical to writing y = y[indices].astype(int) \n",
        "# or writing y = (data['species']=='Adelie')[indices].astype(int)"
      ],
      "metadata": {
        "id": "TEJwe-AvLvN2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out y! If everything is implemented correctly, you should see a panda \n",
        "# series full of ones and zeroes\n",
        "print(y) # panda serie made out of 0s and 1s "
      ],
      "metadata": {
        "id": "x9WveIx-1w5g",
        "outputId": "bf75525d-6948-4c97-90e5-791f5fcfaf20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      1\n",
            "1      1\n",
            "2      1\n",
            "4      1\n",
            "5      1\n",
            "      ..\n",
            "270    0\n",
            "272    0\n",
            "273    0\n",
            "274    0\n",
            "275    0\n",
            "Name: species, Length: 274, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a set of binary classification data we can use to train an algorithm.\n",
        "\n",
        "As we saw during our reading, we need to define three things in order to train our algorithm: \n",
        "> $\\cdot$ the type of algorithm we will train, \\\\\n",
        "> $\\cdot$ the cost function (which will tell us how close our prediction is to the truth), and \\\\\n",
        "> $\\cdot$ a method for updating the parameters in our model according to the value of the cost function (e.g., the gradient descent method). \n",
        "\n",
        "Let's begin by defining the type of algorithm we will use. We will train a logistic regression model to differentiate between two classes. A reminder of how the logistic regression algorithm works is given below.\n",
        "<br><br><br>\n",
        "The logistic regression algorithm will thus take an input $t$ that is a linear combination of the features:\n",
        "\n",
        "<a name=\"logit\"></a>\n",
        "\n",
        "<center> $t_{\\small{n}} = \\beta_{\\small{0}} + \\beta_{\\small{1}} \\cdot X_{1,n} + \\beta_{\\small{2}} \\cdot X_{2,n}$ </center>\n",
        "\n",
        "where \n",
        "* $n$ is the ID of the sample \n",
        "* $X_{\\small{0}}$ represents the petal length\n",
        "* $X_{\\small{1}}$ represents the petal width\n",
        "\n",
        "This input is then fed into the logistic function, $\\sigma$:\n",
        "\\begin{align} \n",
        "\\sigma: t\\mapsto \\dfrac{1}{1+e^ {-t}}\n",
        "\\end{align}\n",
        "\n",
        "Let's define the logistic function for later use."
      ],
      "metadata": {
        "id": "jvNBaOWZ9fXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q4) Define the logistic function**"
      ],
      "metadata": {
        "id": "PzrhQ2E-zkDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hint - Exponential Function\n",
        "'''\n",
        "Numpy includes the exponential function in its library as numpy.exp\n",
        "https://numpy.org/doc/stable/reference/generated/numpy.exp.html\n",
        "''';\n",
        "\n",
        "np.exp(2);"
      ],
      "metadata": {
        "id": "oBvebKZVDWx0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic(in_val):\n",
        "    # Return the value of the logistic function\n",
        "    out_value = 1/(1+np.exp(-in_val)) # result from the calculation  \n",
        "    return out_value"
      ],
      "metadata": {
        "id": "EdelvJlJzuE5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('the value of exp(1) is:\\n',logistic(1\n",
        "               ),\n",
        "      'the value of exp(8) is:\\n',logistic(8)) # print out several values to evaluate that our function works well."
      ],
      "metadata": {
        "id": "OyaqtWelfh0t",
        "outputId": "67c7f0b2-4d40-4e9a-d70c-3b21d4456273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the value of exp(1) is:\n",
            " 0.7310585786300049 the value of exp(8) is:\n",
            " 0.9996646498695336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the logistic function has been defined, we can plot it (this will help us remember what it looks like!) Run the code below - you won't have to fill anything in for this one üòÄ But feel free to show the code and read through it - some of the functions used can be helpful to you down the line!"
      ],
      "metadata": {
        "id": "WqIkC1wZ0gAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this to plot the logistic function!\n",
        "# Let's generate an array of 20 points with values from -4 to +4 \n",
        "t = np.linspace(-4,4,20)\n",
        "\n",
        "# Initiate a figure and axes object using matplotlib\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Draw the X and Y axes\n",
        "ax.axvline(0, c='black', alpha=1)\n",
        "ax.axhline(0, c='black', alpha=1)\n",
        "\n",
        "# Draw the threshold line (y_val=0,5) and asymptote (y=1)\n",
        "[ax.axhline(y_val, c='black', alpha=0.5, linestyle='dotted') for y_val in (0.5,1)]\n",
        "\n",
        "# Scale things to make the graph look nicer\n",
        "plt.autoscale(axis='x', tight=True)\n",
        "\n",
        "# Plot the logistic function. X values from the t vector, y values from logistic(t)\n",
        "ax.plot(t, logistic(t)); # that's the function we defined earlier\n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$\\\\sigma\\\\  \\\\left(t\\\\right)$')\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "lgt9dI6b9Zwa",
        "outputId": "6fd5c953-11d0-4ac6-e868-8a27e9445fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the logistic function, we define inputs resulting in $\\sigma\\geq0.5$ as belonging to the ***one*** class, and any value below that is considered to belong to the ***zero*** class.\n",
        "\n",
        "We now have a function which lets us map the value of the petal length and width to the class to which the observation belongs (i.e., whether the length and width correspond to Iris Versicolor or Iris Virginica). However, there is a parameter vector **$\\theta$** with a number of parameters that we do not have a value for: <br> $\\theta = [ \\beta_{\\small{0}}, \\beta_{\\small{1}}$, $\\beta_{\\small{2}} ]$"
      ],
      "metadata": {
        "id": "0Ll1PKpjxqLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q5) Set up an array of random numbers between 0 and 1 representing the $\\theta$ vector.**\n"
      ],
      "metadata": {
        "id": "O_lT4EaK2ICa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hints: Random Number Generation \n",
        "''' \n",
        "Random Number Generation\n",
        "Use `rnd_gen`! If you're not sure how to use it, consult the `default_rng` \n",
        "documentation at this address:\n",
        "https://numpy.org/doc/stable/reference/random/generator.html\n",
        "\n",
        "For instance, you may use the `random` method of `rnd_gen`.*\n",
        "''';\n",
        "\n",
        "'''\n",
        "The theta array should have 3 elements in it! \n",
        "''';"
      ],
      "metadata": {
        "id": "4ULzWzd750RT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hint: Code Snipppet\n",
        "'''\n",
        "rnd_gen.random((___,)) # length of array\n",
        "''';"
      ],
      "metadata": {
        "id": "R0uUE4VbEfgt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#theta = np.random.default_rng\n",
        "theta = rnd_gen.random(3)\n",
        "theta"
      ],
      "metadata": {
        "id": "-Vk05y1C2VBs",
        "outputId": "a92cf39c-1b2d-4222-e329-ce14434fe6bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.35452597, 0.97069802, 0.89312112])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to determine whether a set of $\\beta$ values is better than the other, we need to quantify well the values are able to predict the class. This is where the cost function comes in.\n",
        "\n",
        "The cost function, $c$, will return a value close to zero when the prediction, $\\hat{p}$, is correct and a large value when it is wrong. In a binary classification problem, we can use the log loss function. For a single prediction and truth value, it is given by:\n",
        "\\begin{align}\n",
        "        \\text{c}(\\hat{p},y) = \\left\\{\n",
        "        \\begin{array}{cl}\n",
        "        -\\log(\\hat{p})& \\text{if}\\; y=1\\\\\n",
        "        -\\log(1-\\hat{p}) & \\text{if}\\; y=0\n",
        "        \\end{array}\n",
        "        \\right.\n",
        "    \\end{align}\n",
        "\n",
        "However, we want to apply the cost function to an n-dimensional set of predictions and truth values. Thankfully, we can find the average value of the log loss function $J$ for an an-dimensional set of $\\hat{y}$ & $y$ as follows:\n",
        "\n",
        "\\begin{align}\n",
        "        \\text{J}(\\mathbf{\\hat{p}},y) = - \\dfrac{1}{n} \\sum_{i=1}^{n} \n",
        "        \\left[ y_i\\cdot \\log\\left( \\hat{p}_i \\right) \\right] + \n",
        "        \\left[ \\left( 1 - y_i \\right) \\cdot \\log\\left( 1-\\hat{p}_i \\right) \\right]\n",
        "    \\end{align}\n",
        "\n",
        "We now have a formula that can be used to calculate the average cost over the training set of data.\n",
        "\n",
        "Now let's code üíª\n"
      ],
      "metadata": {
        "id": "s8KM_CeF2Ven"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q6) Define a log_loss function that takes in an arbitrarily large set of prediction and truths**\n",
        "\n",
        "*Hint 1: You need to encode the function $J$ above, for which Numpy's functions may be quite convenient (e.g., [`log`](https://numpy.org/doc/stable/reference/generated/numpy.log.html), [`mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html), etc.)*\n",
        "\n",
        "*Hint 2: Asserting the dimensions of the vector is a good way to check that your function is working correctly. [Here's a tutorial on how to use `assert`](https://swcarpentry.github.io/python-novice-inflammation/10-defensive/index.html#assertions). For instance, to assert that two vectors `X` and `y` have the same dimension, you may use:*\n",
        "```\n",
        "assert X.shape==y.shape\n",
        "```"
      ],
      "metadata": {
        "id": "XBLxwlSWMoo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hint: Example code snippet\n",
        "'''\n",
        "J_vector  = -(y * np.log(p_hat + epsilon) + (1-y) * np.log(1-y_hat))\n",
        "J.mean()\n",
        "''';"
      ],
      "metadata": {
        "id": "Jmnzz4h_Cq01"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(p_hat, y, epsilon=1e-7):\n",
        "  \n",
        "  # Begin by calculating the two possibilities for the cost function, i.e.\n",
        "  # 1: -log(p_hat + epsilon), and 2: -log(1- p_hat). We added an epsilon term \n",
        "  # to -log(p_hat) because we can run into mathematical problems if p_hat = 0.\n",
        "  term_1 = -np.log( p_hat + epsilon )\n",
        "  term_2 = -np.log( 1 - p_hat )\n",
        "  \n",
        "  # We can almost calculate J! We'll need to 1) multiply term_1 by y, and \n",
        "  # 2) multiply term_2 by (1-y). We then add the new terms together.\n",
        "  # Calculate the value of the cost function (i.e., what's inside the brackets)\n",
        "  inside_brackets = (y) * term_1 + ( 1-y ) * term_2\n",
        "\n",
        "  #Verify the shape of inside_brackets. \n",
        "  print(f'The size of the term inside the brackets is {inside_brackets.shape}')\n",
        "\n",
        "  # You should have a cost value for each one of your predictions. We won't\n",
        "  # use the individual values, though. We'll aggregate the information from\n",
        "  # all our predictions by calculating the mean! (i.e., 1/n_terms * terms_sum)\n",
        "  # This single value is J\n",
        "  J = inside_brackets.mean()\n",
        "\n",
        "  return J"
      ],
      "metadata": {
        "id": "H5fDeL36EauO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a way of quantifying how good our predictions are. The final thing needed for us to train our algorithm is figuring out a way to update the parameters in a way that improves the average quality of our predictions. \n",
        "\n",
        "<br><br>**Warning**: we'll go into a bit of math below <br><br>\n",
        "\n",
        "Let's look at the change in a single parameter within $\\theta$: $\\beta_1$ (given $X_{1,i} = X_1$, $\\;\\hat{p}_{i} = \\hat{p}$, $\\;y_{i} = y$). If we want to know what the effect of changing the value of $\\beta_1$ will have on the log loss function we can find this with the partial derivative:\n",
        "<center>$\n",
        "        \\dfrac{\\partial J}{\\partial \\beta_1}\n",
        "$</center>\n",
        "\n",
        "This may not seem very helpful by itself - after all, $\\beta_1$ isn't even in the expression of $J$. But if we use the chain rule, we can rewrite the expression as:\n",
        "<center>\n",
        "        $\\dfrac{\\partial J}{\\partial \\hat{p}} \\cdot\n",
        "        \\dfrac{\\partial \\hat{p}}{\\partial \\theta} \\cdot\n",
        "        \\dfrac{\\partial \\theta}{\\partial \\beta_1}$\n",
        "</center>\n",
        "\n",
        "We'll spare you the math (feel free to verify it youself, however!):\n",
        "\n",
        "<center>$\\dfrac{\\partial J}{\\partial \\hat{p}} =  \\dfrac{\\hat{p} - y}{\\hat{p}(1-\\hat{p})}, \\quad\n",
        "        \\dfrac{\\partial \\hat{p}}{\\partial \\theta} = \\hat{p} (1-\\hat{p}), \\quad\n",
        "        \\dfrac{\\partial \\theta}{\\partial \\beta_1} = X_1 $\n",
        "</center>\n",
        "\n",
        "and thus \n",
        "<center>$\n",
        "        \\dfrac{\\partial J}{\\partial \\beta_1} = (\\hat{p} - y) \\cdot X_1\n",
        "$</center>\n",
        "\n",
        "We can calculate the partial derivative for each parameter in $\\theta$ which, as you may have realized, is simply the $\\theta$ gradient of $J$: $\\nabla_{\\theta}(J)$\n",
        "\n",
        "With all of this information, we can now write $\\nabla_{\\theta} J$ in terms of the error, the feature vector, and the number of samples we're training on!\n",
        "\n",
        "<a name=\"grad_eq\"></a>\n",
        "\n",
        "<center>$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\theta^{(k)}}) = \\dfrac{1}{n} \\sum\\limits_{i=1}^{n}{ \\left ( \\hat{p}^{(k)}_{i} - y_{i} \\right ) \\mathbf{X}_{i}}$</center>\n",
        "\n",
        "Note that here $k$ represents the iteration of the parameters we are currently on.\n",
        "\n",
        "We now have a gradient we can calculate and use in the batch gradient descent method! The updated parameters will thus be:\n",
        "\n",
        "<a name=\"grad_descent\"></a>\n",
        "\n",
        "\\begin{align} \n",
        "{\\mathbf{\\theta}^{(k+1)}} = {\\mathbf{\\theta}^{(k)}} - \\eta\\,\\nabla_{\\theta^{(k)}}J(\\theta^{(k)})\n",
        "\\end{align}\n",
        "\n",
        "Where $\\eta$ is the learning rate parameter. It's also worth pointing out that $\\;\\hat{p}^{(k)}_i = \\sigma\\left(\\theta^{(k)}, X_i\\right) $"
      ],
      "metadata": {
        "id": "aO4Bkm1gFV3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to easily calculate the input to the logistic regression, we'll multiply the $\\theta$ vector with the X data, and as we have a non-zero bias  $\\beta_0$ we'd like to have an X matrix whose first column is filled with ones.\n",
        "\n",
        "\\begin{align}\n",
        "    X_{\\small{with\\ bias}} = \\begin{pmatrix}\n",
        "        1 & X_{1,0} & X_{2,0}\\\\\n",
        "        1 & X_{1,1} & X_{2,1}\\\\\n",
        "        &...&\\\\\n",
        "        1 & X_{1,n} & X_{2,n} \n",
        "        \\end{pmatrix}\n",
        "\\end{align}\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "ML4uik7sbdMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q7) Prepare the `X_with_bias` matrix.**"
      ],
      "metadata": {
        "id": "sqwV5qgrisB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hints: Making an an array filled with ones, hints on concatenation\n",
        "\n",
        "'''\n",
        "Making the ones array\n",
        "\n",
        "Making an array with ones and the same number of entries as rows in your input \n",
        "data: You can use numpy.ones( (array_dimensions) ) in order to generate an array with\n",
        "the given array_dimensions shape. e.g., np.ones((4,)) => array([1,1,1,1])\n",
        "\n",
        "Accessing the number of rows: dataframes have the \"shape\" attribute implemented.\n",
        "For our penguin data, the input vector shape should be (274,2) (X=(274,2) actually), and so using\n",
        "shape[0] should return the right length for our ones array\n",
        "''';\n",
        "\n",
        "\n",
        "'''\n",
        "Concatenation\n",
        "\n",
        "You can quickly concatenate your arrays using np.c_[array1,array2]. Note that\n",
        "the order matters, so make sure array1 is the array filled with ones :). Also,\n",
        "np.c_ uses square brackets! [] - you'll get an error if you use regular \n",
        "brackets ().\n",
        "\n",
        "numpy.c_ will automatically understand that the second array is a dataframe - \n",
        "you don't need to worry about transforming it into a numpy array for today!\n",
        "\n",
        "''';"
      ],
      "metadata": {
        "id": "PlipJo530lPp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.ones(4)\n",
        "print(np.ones(4,))"
      ],
      "metadata": {
        "id": "epJ7LWLzJaTK",
        "outputId": "13ad7dcc-4afb-4e32-95e4-1780dcda8625",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the ones array\n",
        "ones_array = np.ones(X.shape[0])  # just as if we'd write np.ones(274,)\n",
        "\n",
        "# Make the x_with_bias matrix\n",
        "x_with_bias = np.c_[ones_array,X]"
      ],
      "metadata": {
        "id": "A_-GIW9cCTFn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ones_array.shape"
      ],
      "metadata": {
        "id": "U_4rK6g-JwO4",
        "outputId": "49a55735-767f-4840-bab9-818c1625bbda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(274,)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)"
      ],
      "metadata": {
        "id": "QlJXFWNuJ2xv",
        "outputId": "6700b7bc-4d38-4904-9d90-e32573deeacd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(274, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print your x with bias matrix to make sure it looks the way it's supposed to\n",
        "print(x_with_bias)"
      ],
      "metadata": {
        "id": "QiqtcQj5IIH6",
        "outputId": "3834570e-9f83-4d40-fb93-b489585893a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.00000000e+00 -6.93460421e-01  9.25727520e-01]\n",
            " [ 1.00000000e+00 -6.16471704e-01  2.80056593e-01]\n",
            " [ 1.00000000e+00 -4.62494268e-01  5.78058559e-01]\n",
            " [ 1.00000000e+00 -1.15539273e+00  1.22372949e+00]\n",
            " [ 1.00000000e+00 -6.54966063e-01  1.86940041e+00]\n",
            " [ 1.00000000e+00 -7.31954780e-01  4.78724570e-01]\n",
            " [ 1.00000000e+00 -6.74213242e-01  1.37273047e+00]\n",
            " [ 1.00000000e+00 -1.65581939e+00  6.27725553e-01]\n",
            " [ 1.00000000e+00 -1.35292218e-01  1.67073244e+00]\n",
            " [ 1.00000000e+00 -9.43673754e-01  1.31055609e-01]\n",
            " [ 1.00000000e+00 -9.43673754e-01  2.30389598e-01]\n",
            " [ 1.00000000e+00 -3.08516833e-01  3.79390581e-01]\n",
            " [ 1.00000000e+00 -7.89696319e-01  2.16740238e+00]\n",
            " [ 1.00000000e+00 -1.55958350e+00  2.11773539e+00]\n",
            " [ 1.00000000e+00 -1.17463991e+00  4.78724570e-01]\n",
            " [ 1.00000000e+00 -7.70449139e-01  1.07472850e+00]\n",
            " [ 1.00000000e+00 -3.90563203e-02  1.91906741e+00]\n",
            " [ 1.00000000e+00 -1.59807786e+00  7.76726537e-01]\n",
            " [ 1.00000000e+00  6.34594960e-01  2.31640336e+00]\n",
            " [ 1.00000000e+00 -9.43673754e-01  7.27059542e-01]\n",
            " [ 1.00000000e+00 -9.62920934e-01  9.25727520e-01]\n",
            " [ 1.00000000e+00 -1.30937016e+00  1.17406249e+00]\n",
            " [ 1.00000000e+00 -8.66685036e-01  6.27725553e-01]\n",
            " [ 1.00000000e+00 -7.51201960e-01  1.80722604e-01]\n",
            " [ 1.00000000e+00 -1.42485324e+00  1.02506151e+00]\n",
            " [ 1.00000000e+00 -4.04752730e-01  8.76060525e-01]\n",
            " [ 1.00000000e+00 -4.23999909e-01  5.28391565e-01]\n",
            " [ 1.00000000e+00 -9.24426575e-01  8.76060525e-01]\n",
            " [ 1.00000000e+00 -4.23999909e-01  1.02506151e+00]\n",
            " [ 1.00000000e+00 -6.16471704e-01 -6.76123683e-02]\n",
            " [ 1.00000000e+00 -1.05915683e+00  6.27725553e-01]\n",
            " [ 1.00000000e+00 -6.16471704e-01  4.78724570e-01]\n",
            " [ 1.00000000e+00 -3.47011191e-01  1.02506151e+00]\n",
            " [ 1.00000000e+00 -1.21313427e+00  8.13886149e-02]\n",
            " [ 1.00000000e+00 -6.74213242e-01  2.11773539e+00]\n",
            " [ 1.00000000e+00 -7.51201960e-01  1.57139845e+00]\n",
            " [ 1.00000000e+00 -9.67978587e-02  8.26393531e-01]\n",
            " [ 1.00000000e+00 -9.82168113e-01  1.22372949e+00]\n",
            " [ 1.00000000e+00 -5.58730165e-01  1.12439550e+00]\n",
            " [ 1.00000000e+00 -1.19388709e+00  5.78058559e-01]\n",
            " [ 1.00000000e+00 -3.66258371e-01  7.76726537e-01]\n",
            " [ 1.00000000e+00 -1.29012298e+00  8.26393531e-01]\n",
            " [ 1.00000000e+00  2.68898551e-01  1.42239746e+00]\n",
            " [ 1.00000000e+00 -1.09765119e+00  3.17216205e-02]\n",
            " [ 1.00000000e+00 -5.97224524e-01  9.75394514e-01]\n",
            " [ 1.00000000e+00 -3.08516833e-01  1.07472850e+00]\n",
            " [ 1.00000000e+00 -1.00141529e+00  1.02506151e+00]\n",
            " [ 1.00000000e+00 -1.29012298e+00  5.28391565e-01]\n",
            " [ 1.00000000e+00 -7.75506792e-02  2.16740238e+00]\n",
            " [ 1.00000000e+00 -5.97224524e-01  4.29057576e-01]\n",
            " [ 1.00000000e+00 -5.00988627e-01  1.02506151e+00]\n",
            " [ 1.00000000e+00 -1.48259478e+00  5.28391565e-01]\n",
            " [ 1.00000000e+00 -1.35292218e-01  1.32306348e+00]\n",
            " [ 1.00000000e+00 -1.57883068e+00  6.27725553e-01]\n",
            " [ 1.00000000e+00 -2.50775294e-01  8.76060525e-01]\n",
            " [ 1.00000000e+00 -7.12707601e-01  3.29723587e-01]\n",
            " [ 1.00000000e+00 -4.04752730e-01  9.75394514e-01]\n",
            " [ 1.00000000e+00 -1.19388709e+00 -1.17279363e-01]\n",
            " [ 1.00000000e+00 -9.82168113e-01  1.12439550e+00]\n",
            " [ 1.00000000e+00 -1.34786452e+00  3.17216205e-02]\n",
            " [ 1.00000000e+00 -2.70022474e-01  2.11773539e+00]\n",
            " [ 1.00000000e+00 -9.82168113e-01  8.13886149e-02]\n",
            " [ 1.00000000e+00 -3.08516833e-01  6.77392548e-01]\n",
            " [ 1.00000000e+00 -1.21313427e+00  1.31055609e-01]\n",
            " [ 1.00000000e+00 -2.12280935e-01  5.78058559e-01]\n",
            " [ 1.00000000e+00 -1.38635888e+00 -3.15947340e-01]\n",
            " [ 1.00000000e+00 -3.08516833e-01  1.12439550e+00]\n",
            " [ 1.00000000e+00 -1.30937016e+00 -1.17279363e-01]\n",
            " [ 1.00000000e+00 -1.73786576e-01  1.27339648e+00]\n",
            " [ 1.00000000e+00 -1.77130247e+00  1.07472850e+00]\n",
            " [ 1.00000000e+00 -5.77977345e-01  7.76726537e-01]\n",
            " [ 1.00000000e+00 -5.97224524e-01  1.80722604e-01]\n",
            " [ 1.00000000e+00  5.96100601e-01  1.02506151e+00]\n",
            " [ 1.00000000e+00 -1.38635888e+00  3.29723587e-01]\n",
            " [ 1.00000000e+00  1.86852180e-02  8.26393531e-01]\n",
            " [ 1.00000000e+00 -3.47011191e-01 -1.79453739e-02]\n",
            " [ 1.00000000e+00 -1.05915683e+00  1.27339648e+00]\n",
            " [ 1.00000000e+00 -1.25162863e+00 -3.65614335e-01]\n",
            " [ 1.00000000e+00 -1.16045038e-01  1.12439550e+00]\n",
            " [ 1.00000000e+00 -1.55958350e+00  1.80722604e-01]\n",
            " [ 1.00000000e+00  3.79323974e-02  3.79390581e-01]\n",
            " [ 1.00000000e+00 -1.15539273e+00  9.75394514e-01]\n",
            " [ 1.00000000e+00 -1.46334760e+00  1.27339648e+00]\n",
            " [ 1.00000000e+00 -1.03990965e+00  4.78724570e-01]\n",
            " [ 1.00000000e+00 -2.70022474e-01  1.72039943e+00]\n",
            " [ 1.00000000e+00 -1.23238145e+00  1.32306348e+00]\n",
            " [ 1.00000000e+00 -1.11689837e+00  8.76060525e-01]\n",
            " [ 1.00000000e+00 -8.47437857e-01  1.17406249e+00]\n",
            " [ 1.00000000e+00 -7.31954780e-01  9.75394514e-01]\n",
            " [ 1.00000000e+00 -1.34786452e+00  5.78058559e-01]\n",
            " [ 1.00000000e+00 -3.08516833e-01  6.27725553e-01]\n",
            " [ 1.00000000e+00 -1.67506657e+00  1.31055609e-01]\n",
            " [ 1.00000000e+00 -5.97224524e-01  6.27725553e-01]\n",
            " [ 1.00000000e+00 -1.25162863e+00  2.30389598e-01]\n",
            " [ 1.00000000e+00 -3.66258371e-01  1.02506151e+00]\n",
            " [ 1.00000000e+00 -8.85932216e-01  8.76060525e-01]\n",
            " [ 1.00000000e+00 -4.62494268e-01  8.26393531e-01]\n",
            " [ 1.00000000e+00 -1.84829119e+00 -3.65614335e-01]\n",
            " [ 1.00000000e+00  9.56739358e-02  8.26393531e-01]\n",
            " [ 1.00000000e+00 -1.48259478e+00  5.28391565e-01]\n",
            " [ 1.00000000e+00 -3.27764012e-01  1.57139845e+00]\n",
            " [ 1.00000000e+00 -9.62920934e-01 -4.15281329e-01]\n",
            " [ 1.00000000e+00 -9.43673754e-01  1.57139845e+00]\n",
            " [ 1.00000000e+00 -9.24426575e-01  8.76060525e-01]\n",
            " [ 1.00000000e+00 -5.77977345e-01  1.02506151e+00]\n",
            " [ 1.00000000e+00 -7.89696319e-01  1.80722604e-01]\n",
            " [ 1.00000000e+00 -8.66685036e-01  1.57139845e+00]\n",
            " [ 1.00000000e+00 -8.85932216e-01  8.13886149e-02]\n",
            " [ 1.00000000e+00  9.56739358e-02  1.07472850e+00]\n",
            " [ 1.00000000e+00 -8.85932216e-01 -1.66946357e-01]\n",
            " [ 1.00000000e+00  5.57606242e-01  1.72039943e+00]\n",
            " [ 1.00000000e+00 -5.77977345e-01  4.29057576e-01]\n",
            " [ 1.00000000e+00 -9.67978587e-02  1.32306348e+00]\n",
            " [ 1.00000000e+00 -5.97224524e-01  1.91906741e+00]\n",
            " [ 1.00000000e+00 -5.61961444e-04  7.27059542e-01]\n",
            " [ 1.00000000e+00 -7.89696319e-01  8.13886149e-02]\n",
            " [ 1.00000000e+00 -1.03990965e+00  1.81973342e+00]\n",
            " [ 1.00000000e+00 -1.34786452e+00  8.13886149e-02]\n",
            " [ 1.00000000e+00 -3.08516833e-01  8.76060525e-01]\n",
            " [ 1.00000000e+00 -1.25162863e+00  1.80722604e-01]\n",
            " [ 1.00000000e+00 -9.62920934e-01  1.47206446e+00]\n",
            " [ 1.00000000e+00 -4.81741448e-01  8.13886149e-02]\n",
            " [ 1.00000000e+00 -2.50775294e-01  8.26393531e-01]\n",
            " [ 1.00000000e+00 -1.44410042e+00 -4.64948324e-01]\n",
            " [ 1.00000000e+00 -4.04752730e-01  1.07472850e+00]\n",
            " [ 1.00000000e+00 -7.51201960e-01  3.79390581e-01]\n",
            " [ 1.00000000e+00 -2.31528115e-01  7.27059542e-01]\n",
            " [ 1.00000000e+00 -7.12707601e-01  1.31055609e-01]\n",
            " [ 1.00000000e+00  2.68898551e-01  5.78058559e-01]\n",
            " [ 1.00000000e+00 -8.08943498e-01  5.28391565e-01]\n",
            " [ 1.00000000e+00  7.64267563e-02  1.17406249e+00]\n",
            " [ 1.00000000e+00 -1.13614555e+00  8.26393531e-01]\n",
            " [ 1.00000000e+00 -1.00141529e+00  8.26393531e-01]\n",
            " [ 1.00000000e+00 -8.85932216e-01  3.79390581e-01]\n",
            " [ 1.00000000e+00 -3.08516833e-01  3.29723587e-01]\n",
            " [ 1.00000000e+00 -1.36711170e+00  3.29723587e-01]\n",
            " [ 1.00000000e+00 -4.81741448e-01  1.62106544e+00]\n",
            " [ 1.00000000e+00 -1.09765119e+00 -1.66946357e-01]\n",
            " [ 1.00000000e+00 -5.77977345e-01  5.28391565e-01]\n",
            " [ 1.00000000e+00 -4.81741448e-01  1.31055609e-01]\n",
            " [ 1.00000000e+00 -4.04752730e-01  1.80722604e-01]\n",
            " [ 1.00000000e+00 -2.04076298e+00 -6.63616301e-01]\n",
            " [ 1.00000000e+00 -3.85505550e-01  8.13886149e-02]\n",
            " [ 1.00000000e+00 -1.03990965e+00 -1.79453739e-02]\n",
            " [ 1.00000000e+00 -7.12707601e-01  9.25727520e-01]\n",
            " [ 1.00000000e+00 -6.74213242e-01  8.76060525e-01]\n",
            " [ 1.00000000e+00 -1.17463991e+00  7.76726537e-01]\n",
            " [ 1.00000000e+00 -1.29012298e+00  4.78724570e-01]\n",
            " [ 1.00000000e+00 -9.43673754e-01  6.27725553e-01]\n",
            " [ 1.00000000e+00 -1.29012298e+00  1.31055609e-01]\n",
            " [ 1.00000000e+00 -2.31528115e-01  8.26393531e-01]\n",
            " [ 1.00000000e+00  6.53842140e-01 -1.80595717e+00]\n",
            " [ 1.00000000e+00  1.40448214e+00 -2.66280346e-01]\n",
            " [ 1.00000000e+00  1.15426881e+00 -1.35895422e+00]\n",
            " [ 1.00000000e+00  1.40448214e+00 -8.12617284e-01]\n",
            " [ 1.00000000e+00  9.42549831e-01 -1.16028625e+00]\n",
            " [ 1.00000000e+00  7.30830857e-01 -1.65695619e+00]\n",
            " [ 1.00000000e+00  5.19111884e-01 -1.11061925e+00]\n",
            " [ 1.00000000e+00  7.69325216e-01 -7.62950290e-01]\n",
            " [ 1.00000000e+00  1.14921115e-01 -1.70662318e+00]\n",
            " [ 1.00000000e+00  7.88572396e-01 -7.13283296e-01]\n",
            " [ 1.00000000e+00 -3.47011191e-01 -1.55762220e+00]\n",
            " [ 1.00000000e+00  1.21201034e+00 -3.65614335e-01]\n",
            " [ 1.00000000e+00  5.38359063e-01 -1.55762220e+00]\n",
            " [ 1.00000000e+00  1.09652727e+00 -1.11061925e+00]\n",
            " [ 1.00000000e+00  5.96100601e-01 -1.11061925e+00]\n",
            " [ 1.00000000e+00  1.26975188e+00 -5.64282312e-01]\n",
            " [ 1.00000000e+00 -1.35292218e-01 -1.65695619e+00]\n",
            " [ 1.00000000e+00  1.25050470e+00 -8.12617284e-01]\n",
            " [ 1.00000000e+00  6.73089319e-01 -1.16028625e+00]\n",
            " [ 1.00000000e+00  1.15426881e+00 -8.62284279e-01]\n",
            " [ 1.00000000e+00  1.44297650e+00 -1.25962023e+00]\n",
            " [ 1.00000000e+00  4.61370345e-01 -1.16028625e+00]\n",
            " [ 1.00000000e+00  7.30830857e-01 -1.16028625e+00]\n",
            " [ 1.00000000e+00  6.92336499e-01 -5.14615318e-01]\n",
            " [ 1.00000000e+00  3.79323974e-02 -1.85562417e+00]\n",
            " [ 1.00000000e+00  6.53842140e-01 -8.62284279e-01]\n",
            " [ 1.00000000e+00  3.45887269e-01 -1.25962023e+00]\n",
            " [ 1.00000000e+00  9.81044190e-01 -9.11951273e-01]\n",
            " [ 1.00000000e+00  1.05803291e+00 -1.25962023e+00]\n",
            " [ 1.00000000e+00  1.40448214e+00 -7.62950290e-01]\n",
            " [ 1.00000000e+00  8.84808293e-01 -7.62950290e-01]\n",
            " [ 1.00000000e+00  1.86852180e-02 -1.30928723e+00]\n",
            " [ 1.00000000e+00  4.61370345e-01 -1.16028625e+00]\n",
            " [ 1.00000000e+00  3.25221136e+00  8.13886149e-02]\n",
            " [ 1.00000000e+00  1.23125752e+00 -1.01128526e+00]\n",
            " [ 1.00000000e+00  1.09652727e+00 -2.66280346e-01]\n",
            " [ 1.00000000e+00 -1.98091409e-02 -1.55762220e+00]\n",
            " [ 1.00000000e+00  3.26640089e-01  2.30389598e-01]\n",
            " [ 1.00000000e+00  2.49651371e-01 -1.60728919e+00]\n",
            " [ 1.00000000e+00  1.15426881e+00 -5.64282312e-01]\n",
            " [ 1.00000000e+00 -5.61961444e-04 -1.55762220e+00]\n",
            " [ 1.00000000e+00  1.32749342e+00 -4.15281329e-01]\n",
            " [ 1.00000000e+00  4.99864704e-01 -1.55762220e+00]\n",
            " [ 1.00000000e+00  1.32749342e+00 -9.11951273e-01]\n",
            " [ 1.00000000e+00  1.50071804e+00 -4.64948324e-01]\n",
            " [ 1.00000000e+00  1.72662654e-01 -1.45828821e+00]\n",
            " [ 1.00000000e+00  5.38359063e-01 -1.45828821e+00]\n",
            " [ 1.00000000e+00  1.50071804e+00 -4.64948324e-01]\n",
            " [ 1.00000000e+00  4.22875986e-01 -1.75629018e+00]\n",
            " [ 1.00000000e+00  4.80617525e-01 -5.14615318e-01]\n",
            " [ 1.00000000e+00  7.50078037e-01 -1.30928723e+00]\n",
            " [ 1.00000000e+00  1.11577445e+00 -1.35895422e+00]\n",
            " [ 1.00000000e+00  4.61370345e-01 -1.20995324e+00]\n",
            " [ 1.00000000e+00  1.42372932e+00 -9.11951273e-01]\n",
            " [ 1.00000000e+00  7.30830857e-01 -1.20995324e+00]\n",
            " [ 1.00000000e+00  4.42123166e-01 -7.13283296e-01]\n",
            " [ 1.00000000e+00  2.11157012e-01 -1.45828821e+00]\n",
            " [ 1.00000000e+00  5.38359063e-01 -9.11951273e-01]\n",
            " [ 1.00000000e+00  9.56739358e-02 -1.16028625e+00]\n",
            " [ 1.00000000e+00  1.48147086e+00 -7.62950290e-01]\n",
            " [ 1.00000000e+00  4.99864704e-01 -1.50795521e+00]\n",
            " [ 1.00000000e+00  6.73089319e-01 -9.61618268e-01]\n",
            " [ 1.00000000e+00  5.76853422e-01 -1.45828821e+00]\n",
            " [ 1.00000000e+00  2.23211085e+00 -5.64282312e-01]\n",
            " [ 1.00000000e+00  5.96100601e-01 -1.30928723e+00]\n",
            " [ 1.00000000e+00  1.36598778e+00 -1.79453739e-02]\n",
            " [ 1.00000000e+00  6.73089319e-01 -1.20995324e+00]\n",
            " [ 1.00000000e+00  1.30824624e+00 -3.15947340e-01]\n",
            " [ 1.00000000e+00  1.53415474e-01 -1.30928723e+00]\n",
            " [ 1.00000000e+00  1.53921239e+00 -9.11951273e-01]\n",
            " [ 1.00000000e+00  9.61797011e-01 -9.11951273e-01]\n",
            " [ 1.00000000e+00  7.11583678e-01 -6.13949307e-01]\n",
            " [ 1.00000000e+00  1.05803291e+00 -6.13949307e-01]\n",
            " [ 1.00000000e+00  7.30830857e-01 -1.01128526e+00]\n",
            " [ 1.00000000e+00  7.11583678e-01 -9.11951273e-01]\n",
            " [ 1.00000000e+00  1.13502163e+00 -4.15281329e-01]\n",
            " [ 1.00000000e+00  9.23302652e-01 -1.30928723e+00]\n",
            " [ 1.00000000e+00  1.61620111e+00 -2.66280346e-01]\n",
            " [ 1.00000000e+00  4.80617525e-01 -1.50795521e+00]\n",
            " [ 1.00000000e+00  4.80617525e-01 -2.16613352e-01]\n",
            " [ 1.00000000e+00  1.23125752e+00 -1.16028625e+00]\n",
            " [ 1.00000000e+00  1.88566162e+00 -6.13949307e-01]\n",
            " [ 1.00000000e+00  9.04055472e-01 -1.11061925e+00]\n",
            " [ 1.00000000e+00  1.40448214e+00 -4.64948324e-01]\n",
            " [ 1.00000000e+00  4.22875986e-01 -1.50795521e+00]\n",
            " [ 1.00000000e+00  1.55845957e+00  2.30389598e-01]\n",
            " [ 1.00000000e+00  1.34168295e-01 -1.20995324e+00]\n",
            " [ 1.00000000e+00  1.65469547e+00 -1.30928723e+00]\n",
            " [ 1.00000000e+00  9.23302652e-01 -1.40862122e+00]\n",
            " [ 1.00000000e+00  1.80867291e+00  8.13886149e-02]\n",
            " [ 1.00000000e+00  9.23302652e-01 -9.11951273e-01]\n",
            " [ 1.00000000e+00  1.82792009e+00  1.31055609e-01]\n",
            " [ 1.00000000e+00  5.38359063e-01 -1.16028625e+00]\n",
            " [ 1.00000000e+00  1.30824624e+00 -3.65614335e-01]\n",
            " [ 1.00000000e+00  3.45887269e-01 -1.06095226e+00]\n",
            " [ 1.00000000e+00  1.55845957e+00 -5.64282312e-01]\n",
            " [ 1.00000000e+00  1.28899906e+00 -5.14615318e-01]\n",
            " [ 1.00000000e+00  8.07819575e-01 -1.11061925e+00]\n",
            " [ 1.00000000e+00  1.09652727e+00 -1.20995324e+00]\n",
            " [ 1.00000000e+00  1.61620111e+00 -1.66946357e-01]\n",
            " [ 1.00000000e+00  1.11577445e+00 -9.11951273e-01]\n",
            " [ 1.00000000e+00  2.54006573e+00  8.13886149e-02]\n",
            " [ 1.00000000e+00  8.65561114e-01 -6.63616301e-01]\n",
            " [ 1.00000000e+00  1.23125752e+00 -9.11951273e-01]\n",
            " [ 1.00000000e+00  8.84808293e-01 -1.50795521e+00]\n",
            " [ 1.00000000e+00  7.88572396e-01 -3.65614335e-01]\n",
            " [ 1.00000000e+00 -1.93033756e-01 -1.06095226e+00]\n",
            " [ 1.00000000e+00  2.05888624e+00 -5.14615318e-01]\n",
            " [ 1.00000000e+00  1.14921115e-01 -1.40862122e+00]\n",
            " [ 1.00000000e+00  1.03878573e+00 -8.62284279e-01]\n",
            " [ 1.00000000e+00  1.50071804e+00 -8.12617284e-01]\n",
            " [ 1.00000000e+00  1.36598778e+00 -4.64948324e-01]\n",
            " [ 1.00000000e+00  1.53415474e-01 -8.12617284e-01]\n",
            " [ 1.00000000e+00  1.69318983e+00 -2.66280346e-01]\n",
            " [ 1.00000000e+00  6.73089319e-01 -1.35895422e+00]\n",
            " [ 1.00000000e+00  2.38608829e+00 -4.15281329e-01]\n",
            " [ 1.00000000e+00  3.45887269e-01 -5.64282312e-01]\n",
            " [ 1.00000000e+00  1.17351598e+00 -3.15947340e-01]\n",
            " [ 1.00000000e+00  8.65561114e-01 -1.55762220e+00]\n",
            " [ 1.00000000e+00  7.88572396e-01 -1.25962023e+00]\n",
            " [ 1.00000000e+00  1.48147086e+00 -5.64282312e-01]\n",
            " [ 1.00000000e+00  4.80617525e-01 -1.01128526e+00]\n",
            " [ 1.00000000e+00  1.38523496e+00 -3.65614335e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q8) Write a function called `predict` that takes in the parameter vector $\\theta$ and the `X_with_bias` matrix and evaluates the logistic function for each of the samples.**"
      ],
      "metadata": {
        "id": "cThmFWcB0v-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hint: Pseudocode Snippet\n",
        "\n",
        "'''\n",
        "Pseudocode below:\n",
        "\n",
        "define predict_function(x_with_bias, theta_vector):\n",
        "  argument_for_logistic_function = dot_product(x_with_bias, theta_vector)\n",
        "  return logistic_function(argument_for_logistic_function)\n",
        "\n",
        "''';"
      ],
      "metadata": {
        "id": "opqgiTM2L0jt"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your predict function here\n",
        "def predict_function(x_with_bias, theta):\n",
        "    # Find the dot product of X_with_bias and theta\n",
        "    dot_product = np.dot(x_with_bias,theta) # expression of the dot product\n",
        "\n",
        "    # Use your logistic function!\n",
        "    output = logistic(dot_product)          # use the defined function\n",
        "\n",
        "    return output                           # Return the value you get"
      ],
      "metadata": {
        "id": "tBLryApsbatR"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test your predict function!\n",
        "\n",
        "# Set up debug data and parameters\n",
        "debug_data = np.c_[np.ones(5), np.linspace(-1,1,10).reshape((-1,2))]\n",
        "debug_theta = np.array([0.2,0.1,0.9]) \n",
        "\n",
        "print(predict_function(debug_data, debug_theta)) # it worked! we obtained [0.35434369 0.46118934 0.57172409 0.67553632 0.76454801]"
      ],
      "metadata": {
        "id": "6QIweEz8OKPO",
        "outputId": "86a405c6-e031-46ff-f63b-bc7ed0bfd2d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.35434369 0.46118934 0.57172409 0.67553632 0.76454801]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything is set up correctly and you didn't change the debug data and theta, the output for your predict function should be:\n",
        "\n",
        "`[0.35434369 0.46118934 0.57172409 0.67553632 0.76454801]`"
      ],
      "metadata": {
        "id": "HQ6oK6ewOcHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q9) Now that you have a `predict` function, write a `gradient_calc` function that calculates the gradient for the logistic function.**\n",
        "\n",
        "*Hint: You'll have to feed `theta`, `X`, and `y` to the `gradient_calc` function.*\n",
        "\n",
        "*Hint: You can use [this equation](#grad_eq) to calculate the gradient of the cost function.*"
      ],
      "metadata": {
        "id": "p6cPbu4LvVES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hint: Pseudocode Snippet\n",
        "\n",
        "'''\n",
        "\n",
        "define gradient_calculator_function(y, X_with_bias, theta_vector):\n",
        "  # predicted values using theta and inputs\n",
        "  prediction = predict(x_with_bias,theta_vector)\n",
        "  \n",
        "  number_of_predictions = len(prediction)\n",
        "\n",
        "  assert number_of_predictions == len(y)\n",
        "\n",
        "  error = prediction - y\n",
        "\n",
        "  X_transpose = transpose(X)\n",
        "\n",
        "  return dot_product(X_transpose, error) / number_of_predictions\n",
        "\n",
        "''';\n",
        "\n"
      ],
      "metadata": {
        "id": "XRQNz-2nPGVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.linalg.matfuncs import transpose\n",
        "def gradient_calculator(y, x_with_bias, theta):     # those are the 3 arguments put in the function\n",
        "    # Find predicted values using the predict function\n",
        "    prediction = predict_function(x_with_bias, theta) # function defined before\n",
        "\n",
        "    # Assert that you have the same number of predictions as you do targets\n",
        "    # Otherwise, something went wrong!\n",
        "    assert len(prediction) == len(y)         # number of predictions = number of y (reminder y == 'Adelie' == True)\n",
        "\n",
        "    # Calculate the error\n",
        "    error = prediction - y                   # by how much do we actually derive\n",
        "\n",
        "    # Find the dot product with the input matrix and divide by the number of \n",
        "    # predictions\n",
        "    output = (np.dot(transpose(x_with_bias), error))/len(prediction) \n",
        "    return output"
      ],
      "metadata": {
        "id": "BtnANN5WvVuy"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test the gradient calculator\n",
        "# Begin by creating dummy labels\n",
        "debug_labels = np.array([0,0,0,1,1])\n",
        "\n",
        "# And call the function you defined with the dummy labels and data we made before\n",
        "print(gradient_calculator(debug_labels, debug_data, debug_theta))\n",
        "#print(gradient_calculator(debug_theta, debug_data, debug_labels ))\n",
        "#output [ 0.16546829 -0.19307376 -0.15630302] OK!"
      ],
      "metadata": {
        "id": "sMs0pmFcVo1h",
        "outputId": "962fec7d-b276-44f7-f84f-bba79f532f09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.16546829 -0.19307376 -0.15630302]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "debug_labels.shape"
      ],
      "metadata": {
        "id": "ozRIvbKJWDnI",
        "outputId": "c47e5a1f-9fdc-436e-ebb4-ff2aac740a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5,)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "debug_data.shape"
      ],
      "metadata": {
        "id": "27GkAyNMWHLq",
        "outputId": "e275f5ff-081a-4bec-ecbc-d03ce6e50f0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "debug_theta.shape"
      ],
      "metadata": {
        "id": "oSnaB4opWMzf",
        "outputId": "23ba3f89-d1e6-4945-c4bd-ec2e2c88dbfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you kept the same dummy data we included by default in the notebook, you should get `[ 0.16546829 -0.19307376 -0.15630302]` as the output of your gradient calculator! <font size=+3>üíª</font>"
      ],
      "metadata": {
        "id": "0wc43bGxV-en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now write a function that will train a logistic regression algorithm!\n",
        "\n",
        "Your `logistic_regression` function needs to:\n",
        "* Take in a set of training input/output data, validation input/output data, a number of iterations to train for, a set of initial parameters $\\theta$, and a learning rate $\\eta$\n",
        "* At each iteration:\n",
        " * Generate a set of predictions on the training data. Hint: You may use your function `predict` on inputs `X_train` from the training set.\n",
        " * Calculate and store the loss function for the training data at each iteration. Hint: You may use your function `log_loss` on inputs `X_train` and outputs `y_train` from the training set.\n",
        " * Calculate the gradient. Hint: You may use your function `grad_calc`.\n",
        " * Update the $\\theta$ parameters. Hint: You need to implement [this equation](#grad_descent).\n",
        " * Generate a set of predictions on the validation data using the updated parameters. Hint: You may use your function `predict` on inputs `X_valid` from the validation set. \n",
        " * Calculate and store the loss function for the validation data. Hint: You may use your function `log_loss` on inputs `X_valid` and outputs `y_valid` from the validation set. \n",
        " * Bonus: Calculate and store the accuracy of the model on the training and validation data as a metric!\n",
        "* Return the final set of parameters $\\theta$ & the stored training/validation loss function values (and the accuracy, if you did the bonus)\n",
        "\n"
      ],
      "metadata": {
        "id": "PU4A5HVKuAGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q10) Write the `logistic_regression` function**"
      ],
      "metadata": {
        "id": "182qtGB7i_vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hint: Pseudocode Snippet\n",
        "\n",
        "'''\n",
        "define logistic_regression(\n",
        "                           X_train,\n",
        "                           y_train,\n",
        "                           X_validation,\n",
        "                           y_validation,\n",
        "                           theta_vector,\n",
        "                           number_of_iterations,\n",
        "                           learning_rate_eta,\n",
        "                          ):\n",
        "  #initialize the list of losses\n",
        "  training_losses = list()\n",
        "  validation_losses = list()\n",
        "\n",
        "  for iteration in range(number_of_iterations):\n",
        "    train_set_predictions = predict(X_train, theta_vector)\n",
        "    train_loss = log_loss(train_set_predictions, y_train)\n",
        "    training_losses.append(train_loss) # perfect\n",
        "\n",
        "    gradient = gradient_calculator(y_train, X_train, theta_vector)\n",
        "    theta_vector = theta_vector - gradient * learning_rate_eta\n",
        "\n",
        "    validation_set_predictions = predict(X_validation, theta_vector)\n",
        "    validation_loss = log_loss(validation_set_predictions, y_validation)\n",
        "    validation_losses.append(validation_loss)\n",
        "\n",
        "    print(Completed (iteration)/(number_of_iterations)*100%)\n",
        "\n",
        "    return [training_losses, validation_losses], theta\n",
        "''';"
      ],
      "metadata": {
        "id": "eNfODgtZYm2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(X_train,           # training input data = X_train\n",
        "                        y_train,           # training output data = y_train\n",
        "                        X_validation,      # validation input data = x_validation\n",
        "                        y_validation,      # validation output data = y_validation\n",
        "                        theta,             # set of initial parameters theta\n",
        "                        num_iters,         # number of iterations\n",
        "                        learning_rate_eta, # learning rate 'eta'\n",
        "                      ):\n",
        "  # Initialize the list of losses\n",
        "  training_losses = list()   # each empty list is going to be filled up along the loop...\n",
        "  validation_losses = list()\n",
        "  \n",
        "  # Loop through as many times as defined in the function call\n",
        "  for iteration in range(num_iters): # for i in as many iteration numbers\n",
        "    \n",
        "    #--------Training-------\n",
        "    # Get predictions on training dataset ; we need:\n",
        "    # ==================== 1) our function predict_function(x_with_bias, theta)\n",
        "    # ==================== 2) our inputs X_train ===============================\n",
        "    train_set_predictions = predict_function(X_train, theta) # that's cool\n",
        "    \n",
        "    # Calculate the loss---- we need:\n",
        "    # ==================== 1) our function log_loss(p_hat, y, epsilon=1e-7)\n",
        "    # ==================== 2) x_train and y_train on which to use log_loss\n",
        "    train_loss = log_loss(X_train, y_train) # xtrain = inputs/ ytrain = outputs\n",
        "\n",
        "    # Add it to the list of training losses to keep track of it\n",
        "    training_losses.append(train_loss\n",
        "                           )\n",
        "    \n",
        "    # Calculate the Gradient which is actually ‚àáŒ∏(k)J(Œ∏(k)); needed is:---------\n",
        "    #====================== 1) function gradient_calculator(y, x_with_bias, theta)\n",
        "    grad = gradient_calculator(y_train, X_train, theta) # here y = ytrain, x = xtrain\n",
        "    \n",
        "    # Find the new value of theta according to Œ∏(k+1)=Œ∏(k)‚àíŒ∑‚àáŒ∏(k)J(Œ∏(k)) (thet new = theta old -...) --> update theta\n",
        "    theta = theta - learning_rate_eta * grad \n",
        "\n",
        "    #--------Validation---------------------------------------------------------\n",
        "    # Get predictions on the validation dataset; needed:\n",
        "    # ===================== 1) use function predict_function(x_with_bias, theta)\n",
        "    # ===================== 2) inputs X_valid from the validation set. \n",
        "    validation_set_predictions = predict_function(X_validation, theta)\n",
        "\n",
        "    # Calculate the validation loss: needed:------------------------------------\n",
        "    # ===================== 1) function log_loss(p_hat, y, epsilon=1e-7);\n",
        "    # ===================== 2) inputs x_valid and outputs y_valid\n",
        "    validation_loss = log_loss(X_validation, y_validation)\n",
        "\n",
        "    # Add it to the list of validation losses to keep track of it\n",
        "    validation_losses.append(validation_loss)\n",
        "    \n",
        "    # Progress Indicator\n",
        "    if (iteration/num_iters * 100) % 5 == 0: # if the number is even... (modulo == 0)\n",
        "      print(f'\\rCompleted {(iteration)/(num_iters)*100}%', end='')\n",
        "  \n",
        "  print('\\rCompleted 100%')\n",
        "  return [training_losses, validation_losses], theta"
      ],
      "metadata": {
        "id": "HDsR5TxPt-0Y"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¬°¬°¬°Important Note!!!**\n",
        "\n",
        "The notebook assumes that you will return \n",
        "1. a Losses list, where Losses[0] is the training loss and Losses[1] is the validation loss\n",
        "2. a tuple with the 3 final coefficients ($\\beta_0$, $\\beta_1$, $\\beta_2$)\n",
        "\n",
        "---------------------"
      ],
      "metadata": {
        "id": "EWMDLk7wFB0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our logistic regression function, we're all set to train our algorithm! Or _are_ we?\n",
        "\n",
        "There's an **important** data step that we've neglected up to this point - we need to **split the data** into the train, validation, and test datasets.\n",
        "\n",
        "<center>train <font size=+3> ‚úÇÔ∏è </font> validation <font size=+3> ‚úÇÔ∏è </font> test</center>"
      ],
      "metadata": {
        "id": "2ep5FQYBmqG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(x_with_bias)\n",
        "\n",
        "test_size       = int(total_size * test_ratio)             # = 274*0.2\n",
        "validation_size = int(total_size * validation_ratio)       # = 274*0.2\n",
        "train_size      = total_size - test_size - validation_size # 274 -274*0.2 -274*0.2\n",
        "\n",
        "rnd_indices = rnd_gen.permutation(total_size)\n",
        "# set y to binary ?\n",
        "\n",
        "X_train = x_with_bias[rnd_indices[:train_size]]\n",
        "y_train = bin_y[rnd_indices[:train_size]]\n",
        "X_valid = x_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = bin_y[rnd_indices[train_size:-test_size]]\n",
        "X_test = x_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = bin_y[rnd_indices[-test_size:]]"
      ],
      "metadata": {
        "id": "CVrXzjYA2iil",
        "outputId": "720f729b-f6c0-40bd-d802-5e5989cbeeab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-040a75e1ce11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrnd_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# set y to binary ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbin_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_with_bias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrnd_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbin_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrnd_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Series' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "itlSRf4vh0HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready! \n",
        "\n",
        "###**Q11) Train your logistic regression algorithm. We recommend you use 500 iterations, $\\eta$=0.1**\n",
        "\n",
        "*Hint: It's time to use the `logistic_regression` function you defined in Q5.*"
      ],
      "metadata": {
        "id": "33IhRpME8LOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code\n",
        "losses, coeffs = ________(_______,\n",
        "                          _______,\n",
        "                          _______,\n",
        "                          _______,\n",
        "                          _______,\n",
        "                          _______,\n",
        "                          _______,\n",
        "                          )"
      ],
      "metadata": {
        "id": "dWAr0ORYEYi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how our model did while learning!"
      ],
      "metadata": {
        "id": "e7WHcpPiEcIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this cell to produce the Loss Function Visualization Graphs\n",
        "fig, ax = plt.subplots(figsize=(9,6), dpi=100)\n",
        "ax.plot(losses[0], color='blue', label='Training', linewidth=3);\n",
        "ax.plot(losses[1], color='black', label='Validation', linewidth=3);\n",
        "ax.legend();\n",
        "ax.set_ylabel('Log Loss')\n",
        "ax.set_xlabel('Iterations')\n",
        "ax.set_title('Loss Function Graph')\n",
        "ax.autoscale(axis='x', tight=True)\n",
        "fig.tight_layout();"
      ],
      "metadata": {
        "cellView": "form",
        "id": "T_ImydMTKkfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get predictions from our model for the training, validation, and testing\n",
        "# datasets\n",
        "y_hat_train = (predict(X_train, coeffs)>=.5).astype(int)\n",
        "y_hat_valid = (predict(X_valid, coeffs)>=.5).astype(int)\n",
        "y_hat_test = (predict(X_test, coeffs)>=.5).astype(int)\n",
        "\n",
        "y_sets = [ [y_hat_train, y_train],\n",
        "           [y_hat_valid, y_valid],\n",
        "           [y_hat_test, y_test] ]\n",
        "\n",
        "def accuracy_score(y_hat, y):\n",
        "    assert(y_hat.size==y.size)\n",
        "    return (y_hat == y).sum()/y.size\n",
        "accuracies=[]\n",
        "[accuracies.append(accuracy_score(y_set[0],y_set[1])) for y_set in y_sets]\n",
        "\n",
        "printout= (f'Training Accuracy:{accuracies[0]:.1%} \\n'\n",
        "           f'Validation Accuracy:{accuracies[1]:.1%} \\n'\n",
        "           f'Test Accuracy:{accuracies[1]:.1%} \\n')\n",
        "\n",
        "# Add the testing accuracy only once you're sure that your model works!\n",
        "print(printout)"
      ],
      "metadata": {
        "id": "4wXFzZPjFjOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations on training a logistic regression algorithm from scratch! \n",
        "\n",
        "Your loss function graph should look something similar to this...\n",
        "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/ERvqPkaZswNIoV-RtKx-zvgBAK81ZhYjr__UE-F4EVkDtA?download=1'>\n",
        "\n",
        "And the accuracies we got during development of the notebook are:\n",
        "\n",
        "`Training Accuracy: 68.7%,\n",
        "Validation Accuracy: 51.9%,   \n",
        "Test Accuracy: 51.9% `\n",
        "\n",
        "Once you're done with the upcoming environmental science applications notebook, feel free to come back to take a look at the challenges üòÄ"
      ],
      "metadata": {
        "id": "4zfXs8M8Osie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges\n",
        "\n",
        "* **C1)** Add more features to try to improve our accuracies! \n",
        "\n",
        "* **C2)** Add early stopping to the training algorithm! (e.g., stop training when the accuracy is greater than a target accuracy)"
      ],
      "metadata": {
        "id": "VAa4bzT7PHRG"
      }
    }
  ]
}